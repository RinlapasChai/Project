{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MessageID : จัดทำข้อมูล temp หรือไฟล์ที่ไม่สามารถจับคู่ได้\n",
    "- MessageID : อ่านข้อมูลและจับคู่รวมถึงการ Cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine, update, Table, Column, Integer, String, MetaData, inspect,text\n",
    "import numpy as np\n",
    "import json\n",
    "import pyodbc\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    'dbname': 'import_final',\n",
    "    'user': 'postgres',\n",
    "    'password': '123',\n",
    "    'host': 'localhost',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "postgres_db_config_mo = {\n",
    "    'dbname': '67C_Monitoring',\n",
    "    'user': 'postgres',\n",
    "    'password': 'MyTTT%401234',\n",
    "    'host': '203.154.82.165',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "postgres_db_config_med = {\n",
    "    'dbname': 'meb',\n",
    "    'user': 'postgres',\n",
    "    'password': 'MyTTT%401234',\n",
    "    'host': '203.154.82.165',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "sql_server_config_server = {\n",
    "    'driver': '{SQL Server}',\n",
    "    'server': '203.154.82.165',\n",
    "    'database': 'response',\n",
    "    'user': 'sa',\n",
    "    'password': 'MyTTT@1234'\n",
    "}\n",
    "\n",
    "\n",
    "def setup_postgres_engine(db_config):\n",
    "    try:\n",
    "        db_url = f\"postgresql://{db_config['user']}:{db_config['password']}@{db_config['host']}:{db_config['port']}/{db_config['dbname']}\"\n",
    "        engine = create_engine(db_url)\n",
    "        print(\"PostgreSQL connection successful\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def setup_sql_server_connection(db_config):\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f\"DRIVER={db_config['driver']};\"\n",
    "            f\"SERVER={db_config['server']};\"\n",
    "            f\"DATABASE={db_config['database']};\"\n",
    "            f\"UID={db_config['user']};\"\n",
    "            f\"PWD={db_config['password']};\"\n",
    "        )\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        print(\"SQL Server connection successful\")\n",
    "        return conn\n",
    "    except pyodbc.Error as e:\n",
    "        print(f\"Error connecting to SQL Server: {e}\")\n",
    "        return None\n",
    "        \n",
    "postgres_mybase = setup_postgres_engine(db_config)\n",
    "postgres_67C_Monitoring = setup_postgres_engine(postgres_db_config_mo)\n",
    "postgres_meb = setup_postgres_engine(postgres_db_config_med)\n",
    "sql_server_resp = setup_sql_server_connection(sql_server_config_server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_query = \"SELECT * FROM import_control\"\n",
    "import_control_df = pd.read_sql(control_query, postgres_mybase) \n",
    "\n",
    "control_query = \"SELECT * FROM response_import\"\n",
    "response_import_df = pd.read_sql(control_query, postgres_mybase) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#หา FK ของ ic_id (res_ref_id)\n",
    "df_map = import_control_df.set_index('ic_reference_number')['ic_id'].to_dict()\n",
    "response_import_df['res_ref_id'] = response_import_df['res_ref_no'].map(df_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df_cancel_import_not_null['cc_document_number'].isin(df_response_XCD_message_id_not_null['documentnumber'])\n",
    "filtered_import_cancel_df = df_cancel_import_not_null[condition]\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['cc_is_match'] = 't' if group['cc_messageid'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['cc_is_match'] = 't' if group['cc_messageid'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'ic_reference_number'\n",
    "filtered_import_cancel_df = filtered_import_cancel_df.groupby('cc_reference_number').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "mapping_send_cancel = filtered_import_cancel_df[filtered_import_cancel_df['cc_is_match'] == 't']\n",
    "not_mapping_send_cancel = pd.concat([\n",
    "    df_cancel_import_not_null[~condition],\n",
    "    filtered_import_cancel_df[filtered_import_cancel_df['cc_is_match'] == 'f']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert 'ic_created_date' to datetime and create 'ic_is_use' based on 'ic_ms_messageid' being not null\n",
    "import_control_df['ic_created_date'] = pd.to_datetime(import_control_df['ic_created_date'], errors='coerce')\n",
    "import_control_df['ic_is_use'] = import_control_df['ic_ms_messageid'].notnull()\n",
    "\n",
    "# Step 2: Handle rows with null 'ic_ms_messageid'\n",
    "empty_ic_ms_messageid = import_control_df[import_control_df['ic_ms_messageid'].isnull()]\n",
    "for ic_reference_number, group in empty_ic_ms_messageid.groupby('ic_reference_number'):\n",
    "    if len(group) == 1:\n",
    "        import_control_df.loc[group.index, 'ic_is_use'] = True\n",
    "    else:\n",
    "        most_recent_index = group['ic_created_date'].idxmax()\n",
    "        import_control_df.loc[group.index, 'ic_is_use'] = False\n",
    "        import_control_df.loc[most_recent_index, 'ic_is_use'] = True\n",
    "\n",
    "# Step 3: Convert boolean 'ic_is_use' to string 't' or 'f'\n",
    "import_control_df['ic_is_use'] = import_control_df['ic_is_use'].replace({True: 't', False: 'f'})\n",
    "\n",
    "# Step 4: Handle duplicates and mismatches\n",
    "duplicates = import_control_df[import_control_df.duplicated('ic_reference_number', keep=False)]\n",
    "for ic_reference_number, group in duplicates.groupby('ic_reference_number'):\n",
    "    if group['ic_ms_messageid'].isnull().any() and group['ic_ms_messageid'].notnull().any():\n",
    "        import_control_df.loc[group[group['ic_ms_messageid'].isnull()].index, 'ic_is_use'] = 'f'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Match 'ic_ms_messageid' with 'res_message_id' in response_import_df\n",
    "xml_message_ids = set(response_import_df[response_import_df['res_message_id'].notnull()]['res_message_id'])\n",
    "import_control_df.loc[import_control_df['ic_ms_messageid'].isin(xml_message_ids), 'ic_is_use'] = 't'\n",
    "\n",
    "# Handle cases with duplicate 'ic_reference_number' and 'res_message_type' being 'IDCA'\n",
    "duplicate_refs = import_control_df[import_control_df.duplicated('ic_reference_number', keep=False)]['ic_reference_number'].unique()\n",
    "filtered_response_import = response_import_df[response_import_df['res_message_type'] == 'IDCA']\n",
    "\n",
    "# Check for matches and update 'ic_is_use'\n",
    "def check_match(row):\n",
    "    if row['ic_reference_number'] in duplicate_refs and row['ic_ms_messageid']:\n",
    "        match = filtered_response_import[\n",
    "            (filtered_response_import['res_ref_no'] == row['ic_reference_number']) &\n",
    "            (filtered_response_import['res_message_id'] == row['ic_ms_messageid'])\n",
    "        ]\n",
    "        return 't' if not match.empty else 'f'\n",
    "    return row['ic_is_use']\n",
    "\n",
    "import_control_df['ic_is_use'] = import_control_df.apply(check_match, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source XML (send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_query = \"SELECT * FROM import_control\"\n",
    "\n",
    "med_query = \"\"\"SELECT \"fileName\", \"messageId\" FROM meb_sent\"\"\"\n",
    "\n",
    "import_control_df = pd.read_sql(control_query, postgres_mybase) \n",
    "med_df = pd.read_sql(med_query, postgres_meb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary\n",
    "ms_filename_to_id = med_df.set_index('fileName')['messageId'].to_dict()\n",
    "import_control_df['ic_ms_messageid'] = import_control_df['ic_filename'].str.replace(r'\\.sgn$', '', regex=True).map(ms_filename_to_id)\n",
    "import_control_df['ic_filename'] = import_control_df['ic_filename'].str.replace(r'\\.sgn$', '', regex=True) + '.sgn'\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['ic_ms_messageid'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['ic_ms_messageid'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'ic_reference_number'\n",
    "import_control_df = import_control_df.groupby('ic_reference_number').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "import_control_df_not_null = import_control_df[import_control_df['is_match'] == 't']\n",
    "import_control_df_null = import_control_df[import_control_df['is_match'] == 'f']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# กรองหา 'ic_reference_number' ที่ซ้ำกันและไม่มี 'ic_ms_messageid' สักตัว (ทุกค่าเป็น null)\n",
    "grouped = import_control_df_not_null.groupby('ic_reference_number')\n",
    "\n",
    "duplicatenot_import_control_df_not_null = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['ic_ms_messageid'].isnull().all()\n",
    ")['ic_reference_number'].unique()\n",
    "\n",
    "print(\"Duplicate 'ic_reference_number' without any 'ic_ms_messageid':\")\n",
    "print(duplicatenot_import_control_df_not_null)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source XML (Resonse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sql_resp = \"\"\"\n",
    "        SELECT \"FileName\", \"InboxId\" \n",
    "        FROM response;\"\"\"\n",
    "\n",
    "query_meb_inbox = \"\"\"\n",
    "        SELECT \"id\", \"reftomessageid\", \"messagetimestamp\" \n",
    "        FROM meb_inbox;\"\"\"\n",
    "\n",
    "res_query = \"\"\"SELECT *,  res_data->>'DocumentNumber' AS DocumentNumber FROM response_import;\"\"\" \n",
    "\n",
    "df_sql_resp = pd.read_sql(query_sql_resp, sql_server_resp)\n",
    "df_response_import = pd.read_sql(res_query, postgres_mybase) \n",
    "df_meb_inbox = pd.read_sql(query_meb_inbox, postgres_meb)\n",
    "\n",
    "df_sql_resp = df_sql_resp.astype(str)\n",
    "df_meb_inbox = df_meb_inbox.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary\n",
    "map_inbox_id = df_sql_resp.set_index('FileName')['InboxId'].to_dict()\n",
    "df_response_import['res_inbox_id'] = df_response_import['res_filename'].map(map_inbox_id)\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['res_inbox_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['res_inbox_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'res_ref_no'\n",
    "df_response_import = df_response_import.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "df_response_import_inbox_id_not_null = df_response_import[df_response_import['is_match'] == 't']\n",
    "df_response_import_inbox_id_null = df_response_import[df_response_import['is_match'] == 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary\n",
    "map_message_id = df_meb_inbox.set_index('id')['reftomessageid'].to_dict()\n",
    "df_response_import_inbox_id_not_null['res_message_id'] = df_response_import_inbox_id_not_null['res_inbox_id'].map(map_message_id)\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['res_message_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['res_message_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'res_ref_no'\n",
    "df_response_import_inbox_id_not_null = df_response_import_inbox_id_not_null.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "df_response_import_message_id_not_null = df_response_import_inbox_id_not_null[df_response_import_inbox_id_not_null['is_match'] == 't']\n",
    "df_response_import_message_id_null = df_response_import_inbox_id_not_null[df_response_import_inbox_id_not_null['is_match'] == 'f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping messageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter import_control_df_not_null based on the condition\n",
    "condition = (\n",
    "    import_control_df_not_null['ic_ms_messageid'].isin(df_response_import_message_id_not_null['res_message_id']) &\n",
    "    import_control_df_not_null['ic_reference_number'].isin(df_response_import_message_id_not_null['res_ref_no'])\n",
    ")\n",
    "\n",
    "# Applying the condition to filter rows\n",
    "filtered_import_control_df = import_control_df_not_null[condition]\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['ic_is_match'] = 't' if group['ic_ms_messageid'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['ic_is_match'] = 't' if group['ic_ms_messageid'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'ic_reference_number'\n",
    "filtered_import_control_df = filtered_import_control_df.groupby('ic_reference_number').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "mapping_send = filtered_import_control_df[filtered_import_control_df['ic_is_match'] == 't']\n",
    "not_mapping_send = pd.concat([\n",
    "    import_control_df_not_null[~condition],\n",
    "    filtered_import_control_df[filtered_import_control_df['ic_is_match'] == 'f']\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_mapping_send_import = not_mapping_send.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# กรองหา 'ic_reference_number' ที่ซ้ำกันและไม่มี 'ic_ms_messageid' สักตัว (ทุกค่าเป็น null)\n",
    "grouped = mapping_send.groupby('ic_reference_number')\n",
    "\n",
    "duplicatenot_mapping_send = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['ic_ms_messageid'].isnull().all()\n",
    ")['ic_reference_number'].unique()\n",
    "\n",
    "print(\"Duplicate 'ic_reference_number' without any 'ic_ms_messageid':\")\n",
    "print(duplicatenot_mapping_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter import_control_df_not_null based on the condition\n",
    "condition = (\n",
    "    df_response_import_message_id_not_null['res_message_id'].isin(import_control_df_not_null['ic_ms_messageid']) &\n",
    "    df_response_import_message_id_not_null['res_ref_no'].isin(import_control_df_not_null['ic_reference_number'])\n",
    ")\n",
    "\n",
    "# Applying the condition to filter rows\n",
    "filtered_response_import_df = df_response_import_message_id_not_null[condition]\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['res_is_match'] = 't' if group['res_message_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['res_is_match'] = 't' if group['res_message_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "filtered_response_import_df = filtered_response_import_df.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "mapping_response = filtered_response_import_df[filtered_response_import_df['res_is_match'] == 't']\n",
    "not_mapping_response = pd.concat([\n",
    "    df_response_import_message_id_not_null[~condition],\n",
    "    filtered_response_import_df[filtered_response_import_df['res_is_match'] == 'f']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_mapping_response_import = not_mapping_response.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# กรองหา 'res_ref_no' ที่ซ้ำกันและไม่มี 'res_message_id' สักตัว (ทุกค่าเป็น null)\n",
    "grouped = not_mapping_response.groupby('res_ref_no')\n",
    "\n",
    "duplicatenot_mapping_response = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['res_message_id'].isnull().all()\n",
    ")['res_ref_no'].unique()\n",
    "\n",
    "print(\"Duplicate 'res_ref_no' without any 'res_message_id':\")\n",
    "print(duplicatenot_mapping_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration No. (PMTA,PMTR) : หา ic_rs_id กับ ic_declaration_no ไป update import_control(send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT rs_id, rs_type, rs_message, rs_status_id, rs_status_priority \n",
    "        FROM response_status\n",
    "        WHERE rs_declaration_type = 'Import';\"\"\"\n",
    "        \n",
    "df_response_status = pd.read_sql(query, postgres_67C_Monitoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_PAT = df_response_import[df_response_import['res_message_type'].isin(['PMTA', 'PMTR'])]\n",
    "\n",
    "# rs_type, rs_message จับคุ่และมีค่าซ้ำกันไปดูที่ rs_status_priority เอามากที่สุด\n",
    "df_response_status = df_response_status[df_response_status['rs_type'].isin(['PMTA', 'PMTR'])] \n",
    "df_response_status = df_response_status.loc[df_response_status.groupby(['rs_type', 'rs_message'])['rs_status_priority'].idxmax()]\n",
    "\n",
    "merged_status = pd.merge(df_response_PAT, df_response_status, \n",
    "                        left_on=['res_message_type', 'res_message'], \n",
    "                        right_on=['rs_type', 'rs_message'],\n",
    "                        how='left') \n",
    "\n",
    "# ใช้ groupby ตาม res_ref_no และเอาเพื่อเลือกค่าแรกที่เจอ ถ้า rs_status_priority เท่ากัน \n",
    "ic_rs_id_map = merged_status.groupby('res_ref_no')['rs_id'].first()\n",
    "ic_declaration_no_map = merged_status.groupby('res_ref_no')['res_declaration_number'].first()\n",
    "\n",
    "import_control_df['ic_rs_id'] = import_control_df['ic_reference_number'].map(ic_rs_id_map)\n",
    "import_control_df['ic_declaration_no'] = import_control_df['ic_reference_number'].map(ic_declaration_no_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cancel : ส่ง หา filename จาก cancel ไปหาที่ meb_sent เพื่อเอา messagid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_query = \"\"\" SELECT * \n",
    "                FROM cancel_09 \n",
    "                WHERE cc_declaration_type = 'IMP';\"\"\"\n",
    "df_cancel_import = pd.read_sql(res_query, postgres_mybase) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary\n",
    "ms_filename_to_id = med_df.set_index('fileName')['messageId'].to_dict()\n",
    "df_cancel_import['cc_messageid'] = df_cancel_import['cc_filename'].str.replace(r'\\.sgn$', '', regex=True).map(ms_filename_to_id)\n",
    "df_cancel_import['cc_filename'] = df_cancel_import['cc_filename'].str.replace(r'\\.sgn$', '', regex=True) + '.sgn'\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['cc_messageid'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['cc_messageid'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'cc_reference_number'\n",
    "df_cancel_import = df_cancel_import.groupby('cc_reference_number').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "df_cancel_import_not_null = df_cancel_import[df_cancel_import['is_match'] == 't']\n",
    "df_cancel_import_null = df_cancel_import[df_cancel_import['is_match'] == 'f']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "responce (XDCA,XCDR) : ส่ง filename ไปหาที่ response(sql server) ได้ InboxId --> ส่ง InboxId ไปหาที่ meb_inbox ได้ res_message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_XCD = df_response_import[df_response_import['res_message_type'].isin(['XCDA', 'XCDR'])]\n",
    "map_inbox_id = df_sql_resp.set_index('FileName')['InboxId'].to_dict()\n",
    "df_response_XCD['res_inbox_id'] = df_response_XCD['res_filename'].map(map_inbox_id)\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['res_inbox_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['res_inbox_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'res_ref_no'\n",
    "df_response_XCD = df_response_XCD.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "df_response_XCD_inbox_id_not_null = df_response_XCD[df_response_XCD['is_match'] == 't']\n",
    "df_response_XCD_inbox_id_null = df_response_XCD[df_response_XCD['is_match'] == 'f']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary\n",
    "map_message_id = df_meb_inbox.set_index('id')['reftomessageid'].to_dict()\n",
    "df_response_XCD_inbox_id_not_null['res_message_id'] = df_response_XCD_inbox_id_not_null['res_inbox_id'].map(map_message_id)\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['is_match'] = 't' if group['res_message_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['is_match'] = 't' if group['res_message_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'res_ref_no'\n",
    "df_response_XCD_inbox_id_not_null = df_response_XCD_inbox_id_not_null.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "df_response_XCD_message_id_not_null = df_response_XCD_inbox_id_not_null[df_response_XCD_inbox_id_not_null['is_match'] == 't']\n",
    "df_response_XCD_message_id_null = df_response_XCD_inbox_id_not_null[df_response_XCD_inbox_id_not_null['is_match'] == 'f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_response_XCD_message_id_not_null.groupby('res_ref_no')\n",
    "\n",
    "duplicatenot_XCD_message_id = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['res_message_id'].isnull().all()\n",
    ")['res_ref_no'].unique()\n",
    "\n",
    "print(\"Duplicate 'res_ref_no' without any 'res_message_id':\")\n",
    "print(duplicatenot_XCD_message_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map send_cancel and response_cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df_cancel_import_not_null['cc_document_number'].isin(df_response_XCD_message_id_not_null['documentnumber'])\n",
    "filtered_import_cancel_df = df_cancel_import_not_null[condition]\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['cc_is_match'] = 't' if group['cc_messageid'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['cc_is_match'] = 't' if group['cc_messageid'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group by 'ic_reference_number'\n",
    "filtered_import_cancel_df = filtered_import_cancel_df.groupby('cc_reference_number').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "# Initialize lists to hold grouped dataframes\n",
    "mapping_send_cancel = filtered_import_cancel_df[filtered_import_cancel_df['cc_is_match'] == 't']\n",
    "not_mapping_send_cancel = pd.concat([\n",
    "    df_cancel_import_not_null[~condition],\n",
    "    filtered_import_cancel_df[filtered_import_cancel_df['cc_is_match'] == 'f']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = mapping_send_cancel.groupby('cc_reference_number')\n",
    "\n",
    "duplicatenot_mapping_response = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['cc_messageid'].isnull().all()\n",
    ")['cc_reference_number'].unique()\n",
    "\n",
    "print(\"Duplicate 'res_ref_no' without any 'cc_messageid':\")\n",
    "print(duplicatenot_mapping_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df_response_XCD_message_id_not_null['documentnumber'].isin(df_cancel_import_not_null['cc_document_number'])\n",
    "filtered_cancel_response = df_response_XCD_message_id_not_null[condition]\n",
    "\n",
    "# Function to check each group\n",
    "def check_group(group):\n",
    "    if len(group) == 1:  # Unique reference number\n",
    "        group['res_is_match'] = 't' if group['res_message_id'].notnull().all() else 'f'\n",
    "    else:  # Duplicate reference number\n",
    "        group['res_is_match'] = 't' if group['res_message_id'].notnull().any() else 'f'\n",
    "    return group\n",
    "\n",
    "filtered_cancel_response = filtered_cancel_response.groupby('res_ref_no').apply(check_group).reset_index(drop=True)\n",
    "\n",
    "mapping_response_cancel = filtered_cancel_response[filtered_cancel_response['res_is_match'] == 't']\n",
    "not_mapping_response_cancel = pd.concat([\n",
    "    df_response_XCD_message_id_not_null[~condition],\n",
    "    filtered_cancel_response[filtered_cancel_response['res_is_match'] == 'f']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = mapping_response_cancel.groupby('res_ref_no')\n",
    "\n",
    "duplicatenot_mapping_response = grouped.filter(\n",
    "    lambda x: len(x) > 1 and x['res_message_id'].isnull().all()\n",
    ")['res_ref_no'].unique()\n",
    "\n",
    "print(\"Duplicate 'res_ref_no' without any 'res_message_id':\")\n",
    "print(duplicatenot_mapping_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update ic_is_cancel กับ ic_cancel_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_control_df.loc[import_control_df['ic_declaration_no'].isin(df_response_XCD_message_id_not_null['documentnumber']), 'ic_is_cancel'] = 't'\n",
    "\n",
    "# mapping จาก res_message_id ไปยัง cc_cancel_reason\n",
    "res_to_reason = df_cancel_import.set_index('cc_messageid')['cc_cancel_reason'].to_dict()\n",
    "\n",
    "# mapping จาก documentnumber ไปยัง res_message_id\n",
    "doc_to_res = df_response_XCD_message_id_not_null.set_index('documentnumber')['res_message_id'].to_dict()\n",
    "\n",
    "# Update ic_cancel_reason สำหรับแถวที่ ic_is_cancel เป็น 't'\n",
    "import_control_df.loc[import_control_df['ic_is_cancel'] == 't', 'ic_cancel_reason'] = import_control_df['ic_declaration_no'].map(doc_to_res).map(res_to_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaration No. (IDCA,IDCA) : หา ic_rs_id กับ ic_declaration_no ไป update import_control(send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT rs_id, rs_type, rs_message, rs_status_id, rs_status_priority \n",
    "        FROM response_status\n",
    "        WHERE rs_declaration_type = 'Import';\"\"\"\n",
    "        \n",
    "df_response_status = pd.read_sql(query, postgres_67C_Monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_IDA = df_response_import[df_response_import['res_message_type'].isin(['IDCA', 'IDCR'])]\n",
    "\n",
    "# rs_type, rs_message จับคุ่และมีค่าซ้ำกันไปดูที่ rs_status_priority เอามากที่สุด\n",
    "df_response_status = df_response_status[df_response_status['rs_type'].isin(['IDCA', 'IDCR'])] \n",
    "df_response_status = df_response_status.loc[df_response_status.groupby(['rs_type', 'rs_message'])['rs_status_priority'].idxmax()]\n",
    "\n",
    "merged_status = pd.merge(df_response_IDA, df_response_status, \n",
    "                        left_on=['res_message_type', 'res_message'], \n",
    "                        right_on=['rs_type', 'rs_message'],\n",
    "                        how='left') \n",
    "\n",
    "# ใช้ groupby ตาม res_ref_no และเอาเพื่อเลือกค่าแรกที่เจอ ถ้า rs_status_priority เท่ากัน \n",
    "ic_rs_id_map = merged_status.groupby('res_ref_no')['rs_id'].first()\n",
    "ic_declaration_no_map = merged_status.groupby('res_ref_no')['res_declaration_number'].first()\n",
    "\n",
    "import_control_df['ic_rs_id'] = import_control_df['ic_reference_number'].map(ic_rs_id_map)\n",
    "import_control_df['ic_declaration_no'] = import_control_df['ic_reference_number'].map(ic_declaration_no_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_control_df_null 13963\n",
    "\n",
    "# df_response_import_inbox_id_null 0\n",
    "# df_response_import_message_id_null 61654\n",
    "\n",
    "# not_mapping_send 15\n",
    "# not_mapping_response 1860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_col = ['eh_document_name', 'eh_filename', 'eh_type', 'eh_message_id', 'eh_file_date', \n",
    "                'eh_folder', 'eh_message', 'eh_is_use', 'eh_is_match', 'eh_ref_no', 'eh_remark']\n",
    "                \n",
    "control = pd.DataFrame(columns=required_col)\n",
    "inbox = pd.DataFrame(columns=required_col)\n",
    "message_id = pd.DataFrame(columns=required_col)\n",
    "not_mapping_send = pd.DataFrame(columns=required_col)\n",
    "not_mapping_response = pd.DataFrame(columns=required_col)\n",
    "\n",
    "def update_dataframe(df, values_dict):\n",
    "    new_df = pd.DataFrame(columns=df.columns)\n",
    "    for col, value in values_dict.items():\n",
    "        if value is not None:\n",
    "            new_df[col] = value\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_values = {\n",
    "    'eh_document_name' : import_control_df_null['ic_doc_name'],\n",
    "    'eh_filename': import_control_df_null['ic_filename'],\n",
    "    'eh_message_id': import_control_df_null['ic_ms_messageid'],\n",
    "    'eh_file_date': import_control_df_null['ic_created_date'],\n",
    "    'eh_ref_no': import_control_df_null['ic_reference_number'],\n",
    "    'eh_type': 'SEND',\n",
    "    'eh_is_use': 'f',\n",
    "    'eh_remark': 'ไม่พบ MessageId ที่ Meb_Send'\n",
    "}\n",
    "\n",
    "inbox_values = {\n",
    "    'eh_document_name' : df_response_import_inbox_id_null['res_message_type'],\n",
    "    'eh_filename': df_response_import_inbox_id_null['res_filename'],\n",
    "    'eh_message_id': df_response_import_inbox_id_null['res_message_id'],\n",
    "    'eh_file_date': df_response_import_inbox_id_null['res_datetime'],\n",
    "    'eh_ref_no': df_response_import_inbox_id_null['res_ref_no'],\n",
    "    'eh_type': 'RESPONSE',\n",
    "    'eh_is_use': 'f',\n",
    "    'eh_remark': 'ไม่พบ InboxId ที่ Reponse'\n",
    "}\n",
    "\n",
    "message_id_values = {\n",
    "    'eh_document_name' : df_response_import_message_id_null['res_message_type'],\n",
    "    'eh_filename': df_response_import_message_id_null['res_filename'],\n",
    "    'eh_message_id': df_response_import_message_id_null['res_message_id'],\n",
    "    'eh_file_date': df_response_import_message_id_null['res_datetime'],\n",
    "    'eh_ref_no': df_response_import_message_id_null['res_ref_no'],\n",
    "    'eh_type': 'RESPONSE',\n",
    "    'eh_is_use': 'f',\n",
    "    'eh_remark': 'ไม่พบ MessageId ที่ Meb_Inbox'\n",
    "}\n",
    "\n",
    "not_mapping_send_values = {\n",
    "    'eh_document_name' : not_mapping_send_import['ic_doc_name'],\n",
    "    'eh_filename': not_mapping_send_import['ic_filename'],\n",
    "    'eh_message_id': not_mapping_send_import['ic_ms_messageid'],\n",
    "    'eh_file_date': not_mapping_send_import['ic_created_date'],\n",
    "    'eh_ref_no': not_mapping_send_import['ic_reference_number'],\n",
    "    'eh_type': 'SEND',\n",
    "    'eh_is_use': 'f',\n",
    "    'eh_remark': 'ไม่พบ MessageId ที่ตรงกัน'\n",
    "}\n",
    "\n",
    "not_mapping_response_values = {\n",
    "    'eh_document_name' : not_mapping_response_import['res_message_type'],\n",
    "    'eh_filename': not_mapping_response_import['res_filename'],\n",
    "    'eh_message_id': not_mapping_response_import['res_message_id'],\n",
    "    'eh_file_date': not_mapping_response_import['res_datetime'],\n",
    "    'eh_ref_no': not_mapping_response_import['res_ref_no'],\n",
    "    'eh_type': 'RESPONSE',\n",
    "    'eh_is_use': 'f',\n",
    "    'eh_remark': 'ไม่พบ RefMessageId ที่ตรงกัน '\n",
    "}\n",
    "\n",
    "control = update_dataframe(control, control_values)\n",
    "inbox = update_dataframe(inbox, inbox_values)\n",
    "message_id = update_dataframe(message_id, message_id_values)\n",
    "not_mapping_send = update_dataframe(not_mapping_send, not_mapping_send_values)\n",
    "not_mapping_response = update_dataframe(not_mapping_response, not_mapping_response_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control.to_sql('exception_handle', postgres_mybase, if_exists='append', index=False)\n",
    "# inbox.to_sql('exception_handle', postgres_mybase, if_exists='append', index=False)\n",
    "# message_id.to_sql('exception_handle', postgres_mybase, if_exists='append', index=False)\n",
    "# not_mapping_send.to_sql('exception_handle', postgres_mybase, if_exists='append', index=False)\n",
    "# not_mapping_response.to_sql('exception_handle', postgres_mybase, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
